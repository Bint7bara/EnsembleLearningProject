{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bint7bara/EnsembleLearningProject/blob/master/OllamaFaster_Pipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84bf8ca7-22ab-4277-ae83-02902c1407e7",
      "metadata": {
        "id": "84bf8ca7-22ab-4277-ae83-02902c1407e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "#We are processing 9000 PDFs stored in /home/shared/SAMPLEFILES with quotes and spaces in filenames. The goal is to:\n",
        "\n",
        "#Extract text from PDFs efficiently using pymupdf (fast, handles corrupt PDFs).\n",
        "#Run all PDFs against multiple prompts using Ollama (LLaMA 3.2 3B).\n",
        "#Use GPU acceleration for super-fast processing.\n",
        "#Categorize responses into StrongMatch, PartialMatch, or NoMatch.\n",
        "#Save results into structured folders (/home/shared/CategorizedPDFs/).\n",
        "#Export a CSV file containing the filename, prompt used, response, and match category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2125fb7a-dc3c-4a69-b1e3-1e087153eecb",
      "metadata": {
        "id": "2125fb7a-dc3c-4a69-b1e3-1e087153eecb"
      },
      "outputs": [],
      "source": [
        "# Why This is Faster\n",
        "# Uses pymupdf (Fitz) instead of pdfplumber – 3-5x faster\n",
        "# Ray for parallelization – utilizes all available CPU cores\n",
        "# Handles corrupt PDFs gracefully (logs errors, skips bad files)\n",
        "# Expected runtime: 20-40 minutes (instead of 3 hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed6c904-d107-4158-b598-81b5595bfd21",
      "metadata": {
        "id": "7ed6c904-d107-4158-b598-81b5595bfd21",
        "outputId": "16b7bacb-5ce4-4d34-d63e-cd72cb8c0116"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-31 21:26:44,448\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Extracted text from 93 PDFs and saved to extracted_text.json\n",
            "⚠️ Logged 7 failed PDFs in failed_pdfs.log\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Extract Text from PDFs (Handles Errors & Uses OCR)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import fitz  # pymupdf - Fast PDF processing\n",
        "import pytesseract  # OCR for scanned PDFs\n",
        "from pdf2image import convert_from_path  # Converts PDF to images\n",
        "import ray  # Distributed computing\n",
        "\n",
        "# Initialize Ray for Parallel Execution\n",
        "ray.init(ignore_reinit_error=True, num_cpus=os.cpu_count())\n",
        "\n",
        "# Define Paths\n",
        "pdf_dir = \"/home/shared/SAMPLEFILES_100files\"\n",
        "output_file = \"extracted_text.json\"\n",
        "failed_pdfs_file = \"failed_pdfs.log\"\n",
        "reprocessed_pdfs_file = \"reprocessed_pdfs.json\"\n",
        "\n",
        "# Function to clean filenames (strip quotes and spaces)\n",
        "def clean_filename(filename):\n",
        "    return filename.strip().strip('\"')\n",
        "\n",
        "# Function to extract text from PDFs\n",
        "@ray.remote\n",
        "def extract_text_from_pdf(filename):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file using pymupdf (fitz) for faster processing.\n",
        "    \"\"\"\n",
        "    filename = clean_filename(filename)\n",
        "    pdf_path = os.path.join(pdf_dir, filename)\n",
        "\n",
        "    if not os.path.isfile(pdf_path):\n",
        "        return {\"filename\": filename, \"error\": \"File not found\"}\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "\n",
        "        if not text.strip():\n",
        "            return {\"filename\": filename, \"error\": \"No text extracted\"}\n",
        "\n",
        "        # Extract proposal and award numbers using regex\n",
        "        proposal_match = re.search(r\"Proposal\\s*#?\\s*(\\d+)\", text)\n",
        "        award_match = re.search(r\"Award\\s*#?\\s*(\\d+)\", text)\n",
        "\n",
        "        return {\n",
        "            \"filename\": filename,\n",
        "            \"text\": text,\n",
        "            \"proposal_number\": proposal_match.group(1) if proposal_match else None,\n",
        "            \"award_number\": award_match.group(1) if award_match else None\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"filename\": filename, \"error\": str(e)}\n",
        "\n",
        "# Get list of all PDFs, cleaning filenames\n",
        "pdf_files = [clean_filename(f) for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Run text extraction in parallel using Ray\n",
        "results = ray.get([extract_text_from_pdf.remote(f) for f in pdf_files])\n",
        "\n",
        "# Separate successful and failed extractions\n",
        "successful_extractions = [r for r in results if \"error\" not in r]\n",
        "failed_extractions = [r for r in results if \"error\" in r]\n",
        "\n",
        "# Save extracted text to JSON\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(successful_extractions, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Log failed PDFs\n",
        "with open(failed_pdfs_file, \"w\", encoding=\"utf-8\") as log:\n",
        "    for failure in failed_extractions:\n",
        "        log.write(f\"{failure['filename']}: {failure['error']}\\n\")\n",
        "\n",
        "print(f\"✅ Extracted text from {len(successful_extractions)} PDFs and saved to {output_file}\")\n",
        "print(f\"⚠️ Logged {len(failed_extractions)} failed PDFs in {failed_pdfs_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a359cea-d515-4f9e-bbfb-212150cddc73",
      "metadata": {
        "id": "5a359cea-d515-4f9e-bbfb-212150cddc73",
        "outputId": "4d5cb1a9-12c7-4a34-ca43-de1c2bec4c70"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: pdf_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(e)}\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Process failed PDFs using OCR\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m ocr_results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mocr_extract_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfailed_pdfs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Save OCR results\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(reprocessed_pdfs_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "File \u001b[0;32m/opt/miniforge3/envs/jupyter/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniforge3/envs/jupyter/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniforge3/envs/jupyter/lib/python3.12/site-packages/ray/_private/worker.py:2772\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2767\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2768\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2769\u001b[0m     )\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2772\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2774\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
            "File \u001b[0;32m/opt/miniforge3/envs/jupyter/lib/python3.12/site-packages/ray/_private/worker.py:893\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    884\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    885\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    886\u001b[0m         )\n\u001b[1;32m    888\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    890\u001b[0m )\n\u001b[1;32m    891\u001b[0m data_metadata_pairs: List[\n\u001b[1;32m    892\u001b[0m     Tuple[ray\u001b[38;5;241m.\u001b[39m_raylet\u001b[38;5;241m.\u001b[39mBuffer, \u001b[38;5;28mbytes\u001b[39m]\n\u001b[0;32m--> 893\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
            "File \u001b[0;32mpython/ray/_raylet.pyx:3190\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpython/ray/includes/common.pxi:83\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#  Step 2: Process Failed PDFs Using OCR\n",
        "# Load Failed PDFs\n",
        "failed_pdfs = []\n",
        "with open(failed_pdfs_file, \"r\") as f:\n",
        "    failed_pdfs = [clean_filename(line.split(\":\")[0]) for line in f.readlines()]\n",
        "\n",
        "# Function to extract text using OCR\n",
        "@ray.remote\n",
        "def ocr_extract_text(pdf_filename):\n",
        "    \"\"\"\n",
        "    Uses OCR to extract text from scanned PDFs.\n",
        "    Converts PDF pages to images and applies Tesseract OCR.\n",
        "    \"\"\"\n",
        "    pdf_path = os.path.join(pdf_dir, pdf_filename)\n",
        "\n",
        "    if not os.path.isfile(pdf_path):\n",
        "        return {\"filename\": pdf_filename, \"error\": \"File not found\"}\n",
        "\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=300)\n",
        "        extracted_text = \"\\n\".join([pytesseract.image_to_string(img) for img in images])\n",
        "\n",
        "        if not extracted_text.strip():\n",
        "            return {\"filename\": pdf_filename, \"error\": \"OCR failed - No text found\"}\n",
        "\n",
        "        return {\"filename\": pdf_filename, \"text\": extracted_text}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"filename\": pdf_filename, \"error\": str(e)}\n",
        "\n",
        "# Process failed PDFs using OCR\n",
        "ocr_results = ray.get([ocr_extract_text.remote(f) for f in failed_pdfs])\n",
        "\n",
        "# Save OCR results\n",
        "with open(reprocessed_pdfs_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(ocr_results, f, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af57ebb3-cfb1-4300-bad1-40d3860db47c",
      "metadata": {
        "id": "af57ebb3-cfb1-4300-bad1-40d3860db47c",
        "outputId": "d322394b-63ed-40b9-d5d4-55bfe8cb7d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 8 prompts from /home/shared/Prompts\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Load Prompts for Querying\n",
        "\n",
        "prompts_dir = \"/home/shared/Prompts\"\n",
        "prompts = {}\n",
        "\n",
        "for prompt_file in os.listdir(prompts_dir):\n",
        "    prompt_path = os.path.join(prompts_dir, prompt_file)\n",
        "\n",
        "    if os.path.isfile(prompt_path) and prompt_file.endswith(\".txt\"):\n",
        "        with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            prompts[prompt_file] = f.read().strip()\n",
        "\n",
        "print(f\"✅ Loaded {len(prompts)} prompts from {prompts_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c59db23-0b08-477f-9871-b7d5a9815124",
      "metadata": {
        "id": "0c59db23-0b08-477f-9871-b7d5a9815124"
      },
      "outputs": [],
      "source": [
        "# step 4 that extracts confidence scores from llama:\n",
        "\n",
        "import ollama\n",
        "import ray\n",
        "import re  # ✅ Needed to extract confidence score\n",
        "\n",
        "# Initialize Ray for GPU parallel execution\n",
        "ray.init(ignore_reinit_error=True, num_gpus=1)\n",
        "\n",
        "@ray.remote\n",
        "def query_ollama(text, prompt_text, prompt_name, filename):\n",
        "    \"\"\"\n",
        "    Queries Ollama using extracted text and a given prompt.\n",
        "    Runs queries in parallel using Ray.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        messages = [{\"role\": \"system\", \"content\": prompt_text}, {\"role\": \"user\", \"content\": text}]\n",
        "        response = ollama.chat(model=\"llama3.2:3b\", messages=messages)\n",
        "\n",
        "        # ✅ Extract response text safely\n",
        "        response_text = response.get(\"message\", {}).get(\"content\", \"No response from Ollama\").strip()\n",
        "\n",
        "        # ✅ Extract confidence score from response using regex\n",
        "        confidence_match = re.search(r\"confidence score:\\s*(\\d+)\", response_text, re.IGNORECASE)\n",
        "        confidence_score = int(confidence_match.group(1)) if confidence_match else \"N/A\"  # Default to \"N/A\" if missing\n",
        "\n",
        "        return {\n",
        "            \"filename\": filename,\n",
        "            \"prompt\": prompt_name,  # ✅ Store only the prompt name here\n",
        "            \"response\": response_text,  # ✅ Full response from Ollama\n",
        "            \"confidence_score\": confidence_score  # ✅ Extracted confidence score\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"filename\": filename,\n",
        "            \"prompt\": prompt_name,\n",
        "            \"response\": f\"Error: {str(e)}\",\n",
        "            \"confidence_score\": \"N/A\"  # Assign \"N/A\" on failure\n",
        "        }\n",
        "\n",
        "# Run Ollama queries in parallel\n",
        "query_tasks = []\n",
        "for doc in successful_extractions:\n",
        "    for prompt_name, prompt_text in prompts.items():  # ✅ Use prompt_name\n",
        "        query_tasks.append(query_ollama.remote(doc[\"text\"], prompt_text, prompt_name, doc[\"filename\"]))\n",
        "\n",
        "# Retrieve responses from Ray tasks\n",
        "responses = ray.get(query_tasks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda35498-2bd4-450b-8699-e77dad9317c2",
      "metadata": {
        "id": "bda35498-2bd4-450b-8699-e77dad9317c2"
      },
      "outputs": [],
      "source": [
        "# Step 5: Categorizing Responses\n",
        "\n",
        "def categorize_match(response):\n",
        "    response_text = response.lower() if isinstance(response, str) else \"\"\n",
        "\n",
        "    if \"strongly related\" in response_text:\n",
        "        return \"StrongMatch\"\n",
        "    elif \"partially related\" in response_text:\n",
        "        return \"PartialMatch\"\n",
        "    else:\n",
        "        return \"NoMatch\"\n",
        "\n",
        "for res in responses:\n",
        "    res[\"match_status\"] = categorize_match(res[\"response\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c218c30-6ec2-496d-957d-604a16ed6b2e",
      "metadata": {
        "id": "4c218c30-6ec2-496d-957d-604a16ed6b2e",
        "outputId": "ebf34d26-bdb9-4c88-8f5a-95d4d8cd7ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Results saved to CSV: /home/shared/CategorizedPDFs/results.csv\n"
          ]
        }
      ],
      "source": [
        "#step 6\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Define directories for categorized PDFs\n",
        "categorized_dir = \"/home/shared/CategorizedPDFs\"\n",
        "\n",
        "# Ensure directories exist for each category\n",
        "for category in [\"StrongMatch\", \"PartialMatch\", \"NoMatch\"]:\n",
        "    os.makedirs(os.path.join(categorized_dir, category), exist_ok=True)\n",
        "\n",
        "# ✅ Assign numbers to prompts using their short names\n",
        "prompt_numbers = {prompt_name: i+1 for i, prompt_name in enumerate(prompts.keys())}\n",
        "\n",
        "# Create a list to store CSV data\n",
        "csv_data = []\n",
        "\n",
        "for res in responses:\n",
        "    filename = res[\"filename\"]\n",
        "    match_status = res[\"match_status\"]\n",
        "    response_text = res.get(\"response\", \"No response recorded\")  # ✅ Avoids KeyError\n",
        "    prompt_used = res[\"prompt\"]  # ✅ Now contains short prompt name\n",
        "    confidence_score = res.get(\"confidence_score\", \"N/A\")  # ✅ Avoids KeyError if missing\n",
        "\n",
        "    # ✅ Ensure prompt_used is a valid key\n",
        "    prompt_number = prompt_numbers.get(prompt_used, \"N/A\")  # ✅ Prevents KeyError\n",
        "\n",
        "    # Copy PDF into the appropriate folder based on match status\n",
        "    shutil.copy2(os.path.join(pdf_dir, filename), os.path.join(categorized_dir, match_status))\n",
        "\n",
        "    # Append data to CSV list\n",
        "    csv_data.append({\n",
        "        \"Filename\": filename,\n",
        "        \"Prompt Number\": prompt_number,  # ✅ Now works correctly\n",
        "        \"Prompt Used\": prompt_used,  # ✅ Now uses short name\n",
        "        \"Match Status\": match_status,\n",
        "        \"Confidence Score\": confidence_score,  # ✅ No heuristic, only from Ollama\n",
        "        \"Response Text\": response_text  # ✅ Ensures response_text is included\n",
        "    })\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(csv_data)\n",
        "\n",
        "# Save to CSV file\n",
        "csv_output_path = os.path.join(categorized_dir, \"results.csv\")\n",
        "df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "print(f\"✅ Results saved to CSV: {csv_output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628659ae-3448-4c00-8e91-7102cff6889f",
      "metadata": {
        "id": "628659ae-3448-4c00-8e91-7102cff6889f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}